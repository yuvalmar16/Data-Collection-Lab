{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56cbdc5d-d130-4eae-9720-47c4655fe7ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install dlib\n",
    "!pip install protobuf==3.20.3\n",
    "!pip install pytorch_tabnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b35d37-497e-452d-a5b2-6f76471d0024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n  Obtaining dependency information for opencv-python-headless from https://files.pythonhosted.org/packages/dd/5c/c139a7876099916879609372bfa513b7f1257f7f1a908b0bdc1c2328241b/opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.21.2 in /databricks/python3/lib/python3.11/site-packages (from opencv-python-headless) (1.23.5)\nDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/50.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.3/50.0 MB\u001B[0m \u001B[31m10.1 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\n\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.9/50.0 MB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\n\u001B[2K   \u001B[91m━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/50.0 MB\u001B[0m \u001B[31m14.5 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\n\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.3/50.0 MB\u001B[0m \u001B[31m16.5 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\n\u001B[2K   \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/50.0 MB\u001B[0m \u001B[31m19.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\n\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.8/50.0 MB\u001B[0m \u001B[31m22.9 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/50.0 MB\u001B[0m \u001B[31m26.8 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.8/50.0 MB\u001B[0m \u001B[31m31.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.7/50.0 MB\u001B[0m \u001B[31m48.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.3/50.0 MB\u001B[0m \u001B[31m75.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m20.0/50.0 MB\u001B[0m \u001B[31m109.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.3/50.0 MB\u001B[0m \u001B[31m157.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m32.6/50.0 MB\u001B[0m \u001B[31m178.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m38.6/50.0 MB\u001B[0m \u001B[31m173.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m44.4/50.0 MB\u001B[0m \u001B[31m171.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m50.0/50.0 MB\u001B[0m \u001B[31m175.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m50.0/50.0 MB\u001B[0m \u001B[31m175.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m50.0/50.0 MB\u001B[0m \u001B[31m175.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m50.0/50.0 MB\u001B[0m \u001B[31m175.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m50.0/50.0 MB\u001B[0m \u001B[31m175.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m50.0/50.0 MB\u001B[0m \u001B[31m175.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m50.0/50.0 MB\u001B[0m \u001B[31m175.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m50.0/50.0 MB\u001B[0m \u001B[31m175.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m50.0/50.0 MB\u001B[0m \u001B[31m175.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.0/50.0 MB\u001B[0m \u001B[31m24.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: opencv-python-headless\nSuccessfully installed opencv-python-headless-4.11.0.86\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93aefb6c-5869-409c-a72e-f2cecc47f207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ma9eb81j\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ma9eb81j\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting ftfy (from clip==1.0)\n  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/ab/6e/81d47999aebc1b155f81eca4477a616a70f238a2549848c38983f3c22a82/ftfy-6.3.1-py3-none-any.whl.metadata\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from clip==1.0) (23.2)\nRequirement already satisfied: regex in /databricks/python3/lib/python3.11/site-packages (from clip==1.0) (2022.7.9)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.11/site-packages (from clip==1.0) (4.65.0)\nRequirement already satisfied: torch in /databricks/python3/lib/python3.11/site-packages (from clip==1.0) (2.3.1+cpu)\nRequirement already satisfied: torchvision in /databricks/python3/lib/python3.11/site-packages (from clip==1.0) (0.18.1+cpu)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.5)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (3.13.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (4.10.0)\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (1.11.1)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (3.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (2023.5.0)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.23.5)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /databricks/python3/lib/python3.11/site-packages (from torchvision->clip==1.0) (9.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.1)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/44.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.8/44.8 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py): started\n  Building wheel for clip (setup.py): finished with status 'done'\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=6eb73f0473188fdbbec34d45816926a3f853d89ff6aaa206b6727ed6a64e939a\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mifrbl_t/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64660a6-ce20-49a8-ab29-7c4aef196b3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dlib\n  Downloading dlib-19.24.6.tar.gz (3.4 MB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.2/3.4 MB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m54.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m39.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nBuilding wheels for collected packages: dlib\n  Building wheel for dlib (setup.py): started\n  Building wheel for dlib (setup.py): still running...\n  Building wheel for dlib (setup.py): still running...\n  Building wheel for dlib (setup.py): still running...\n  Building wheel for dlib (setup.py): still running...\n  Building wheel for dlib (setup.py): still running...\n  Building wheel for dlib (setup.py): finished with status 'done'\n  Created wheel for dlib: filename=dlib-19.24.6-cp311-cp311-linux_x86_64.whl size=4089508 sha256=6e7d461c455480989cef2266f04c2c74700038323d9f98b80bdbccf275715eef\n  Stored in directory: /root/.cache/pip/wheels/fe/c7/1f/c778b9f7cc6d8d0da4f6697f619f9eb5a49d54d2a2c8267f3c\nSuccessfully built dlib\nInstalling collected packages: dlib\nSuccessfully installed dlib-19.24.6\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "230fac45-818b-462c-981b-9085796f87d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install opencv-python-headless mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5575539b-3ca4-49d4-8996-f3d6b449c36b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8881827304332981>, line 20\u001B[0m\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BytesIO\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlib\u001B[39;00m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmediapipe\u001B[39;00m\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'dlib'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'dlib'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'dlib'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-8881827304332981>, line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BytesIO\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlib\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmediapipe\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'dlib'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision import transforms\n",
    "from IPython.display import FileLink, HTML\n",
    "import clip\n",
    "import logging\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import dlib\n",
    "import mediapipe\n",
    "import math\n",
    "import torch\n",
    "import clip\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ImageProcessingNIMAWithLLM\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a78411-3920-4da9-805c-4b329716c25e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-8jgo6qs6\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-8jgo6qs6\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting ftfy (from clip==1.0)\n  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/ab/6e/81d47999aebc1b155f81eca4477a616a70f238a2549848c38983f3c22a82/ftfy-6.3.1-py3-none-any.whl.metadata\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from clip==1.0) (23.2)\nRequirement already satisfied: regex in /databricks/python3/lib/python3.11/site-packages (from clip==1.0) (2022.7.9)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.11/site-packages (from clip==1.0) (4.65.0)\nRequirement already satisfied: torch in /databricks/python3/lib/python3.11/site-packages (from clip==1.0) (2.3.1+cpu)\nRequirement already satisfied: torchvision in /databricks/python3/lib/python3.11/site-packages (from clip==1.0) (0.18.1+cpu)\nRequirement already satisfied: wcwidth in /databricks/python3/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.5)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (3.13.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (4.10.0)\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (1.11.1)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (3.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.11/site-packages (from torch->clip==1.0) (2023.5.0)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.23.5)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /databricks/python3/lib/python3.11/site-packages (from torchvision->clip==1.0) (9.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.1)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/44.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.8/44.8 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py): started\n  Building wheel for clip (setup.py): finished with status 'done'\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=9cde52ee93269ca5d0bc1e0c135776e91e6c7cb66a0dc35caf682477110a30e0\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ngwsgk2l/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.3.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdff8af5-d1b0-4297-bb9b-ee53588db124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loadning the image folders and merging them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10f06d50-b556-49fd-b01c-52b79dc1ac7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Function to unzip a file and save it in the same folder\n",
    "def unzip_file_in_same_folder(zip_file_path):\n",
    "    # Get the directory of the zip file\n",
    "    extraction_dir = os.path.splitext(zip_file_path)[0]  # Remove .zip extension\n",
    "\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extraction_dir)\n",
    "\n",
    "    print(f\"Unzipped files are saved in: {extraction_dir}\")\n",
    "    return extraction_dir\n",
    "\n",
    "# Example usage\n",
    "zip_file_paths = [\n",
    "    \"/dbfs/FileStore/employee_images3_20250118T091934Z_001.zip\",\n",
    "    \"/dbfs/FileStore/more_employee_images_20250118T091838Z_001.zip\",\n",
    "    \"/dbfs/FileStore/output_images_20250118T092026Z_001.zip\",\n",
    "    \"/dbfs/FileStore/profile_pictures_20250118T091222Z_001.zip\"\n",
    "]\n",
    "\n",
    "for zip_file_path in zip_file_paths:\n",
    "    new_path = unzip_file_in_same_folder(zip_file_path)\n",
    "    print(f\"Unzipped content is located at: {new_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7109f7a4-831e-4dff-906d-dccbf7fb1e36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Function to collect images from directories and save them in a single directory\n",
    "def collect_and_save_images(directories, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for directory in directories:\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.jpg', '.png')):\n",
    "                    source_path = os.path.join(root, file)\n",
    "                    dest_path = os.path.join(output_directory, file)\n",
    "\n",
    "                    # Handle duplicate filenames by adding a suffix\n",
    "                    base, ext = os.path.splitext(file)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(dest_path):\n",
    "                        dest_path = os.path.join(output_directory, f\"{base}_{counter}{ext}\")\n",
    "                        counter += 1\n",
    "\n",
    "                    shutil.copy(source_path, dest_path)\n",
    "\n",
    "# Directories with already unzipped files\n",
    "directories = [\n",
    "    \"/dbfs/FileStore/employee_images3_20250118T091934Z_001/employee_images3\",\n",
    "    \"/dbfs/FileStore/more_employee_images_20250118T091838Z_001/more_employee_images\",\n",
    "    \"/dbfs/FileStore/output_images_20250118T092026Z_001/output_images\",\n",
    "    \"/dbfs/FileStore/profile_pictures_20250118T091222Z_001/profile_pictures\"\n",
    "]\n",
    "\n",
    "# Output directory to save all images\n",
    "output_directory = \"/dbfs/FileStore/all_images_combined\"\n",
    "\n",
    "# Collect and save all images\n",
    "collect_and_save_images(directories, output_directory)\n",
    "\n",
    "print(f\"All images have been saved to: {output_directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abfd4850-23ee-4df6-96ff-ad45dbef208a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start here: the images are already in the DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4b87741-0bd9-41c9-b96c-a940ebfb08e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Output directory to save all images\n",
    "output_directory = \"/dbfs/FileStore/all_images_combined\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"All images have been saved to: {output_directory}\")\n",
    "# Function to display the first N images in a directory with 5 images per row\n",
    "def display_first_n_images(directory, n=10):\n",
    "    try:\n",
    "        image_files = [f for f in os.listdir(directory) if f.lower().endswith(('.jpg', '.png'))][:n]\n",
    "        num_rows = -(-len(image_files) // 5)  # Calculate number of rows, 5 images per row\n",
    "\n",
    "        fig, axes = plt.subplots(num_rows, 5, figsize=(15, 3 * num_rows))\n",
    "        axes = axes.flatten()  # Flatten to handle cases where there are fewer images\n",
    "\n",
    "        for i, image_file in enumerate(image_files):\n",
    "            image_path = os.path.join(directory, image_file)\n",
    "            with Image.open(image_path) as img:\n",
    "                axes[i].imshow(img)\n",
    "                axes[i].axis(\"off\")  # Hide axes for better visualization\n",
    "                axes[i].set_title(f\"Image {i + 1}\")\n",
    "\n",
    "        # Hide any unused subplots\n",
    "        for j in range(len(image_files), len(axes)):\n",
    "            axes[j].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying images: {e}\")\n",
    "# Display the first 10 images\n",
    "display_first_n_images(output_directory, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbcbe9b7-05d8-4147-baef-e7d1abb9a266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize CLIP model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "Clip_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ad91edc-b90b-48c1-83f3-b1cd64e29036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Initialize CLIP model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Clip_model, Clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Function to validate and preprocess an image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            # Handle transparency by converting to RGB\n",
    "            if img.mode in (\"P\", \"RGBA\"):\n",
    "                img = img.convert(\"RGB\")\n",
    "            return Clip_preprocess(img).unsqueeze(0).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to calculate LinkedIn score for an image\n",
    "def calculate_linkedin_score(image_path, clip_model, clip_preprocess, device):\n",
    "    text_prompts = [\"A good profile picture for LinkedIn\", \"A bad profile picture for linkedIn\"]\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image_tensor = load_and_preprocess_image(image_path)\n",
    "    if image_tensor is None:\n",
    "        return None\n",
    "\n",
    "    # Tokenize text prompts\n",
    "    text = clip.tokenize(text_prompts).to(device)\n",
    "\n",
    "    # Compute similarity\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_tensor)\n",
    "        text_features = clip_model.encode_text(text)\n",
    "\n",
    "        # Normalize features\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    # Extract the score for \"Good LinkedIn profile\"\n",
    "    scores = similarities.cpu().numpy().flatten()\n",
    "    linkedin_score = scores[0] * 100\n",
    "    return linkedin_score\n",
    "\n",
    "# Directory containing images\n",
    "output_directory = \"/dbfs/FileStore/all_images_combined\"\n",
    "\n",
    "# Process the first 10 images for LinkedIn scores and display them in a grid\n",
    "image_files = [f for f in os.listdir(output_directory) if f.lower().endswith(('.jpg', '.png'))][:100]\n",
    "num_rows = -(-len(image_files) // 5)  # Calculate number of rows, 5 images per row\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, 5, figsize=(15, 3 * num_rows))\n",
    "axes = axes.flatten()  # Flatten to handle cases where there are fewer images\n",
    "\n",
    "for i, image_file in enumerate(image_files):\n",
    "    image_path = os.path.join(output_directory, image_file)\n",
    "    linkedin_score = calculate_linkedin_score(image_path, Clip_model, Clip_preprocess, device)\n",
    "\n",
    "    # Display the image with the score\n",
    "    if linkedin_score is not None:\n",
    "        with Image.open(image_path) as img:\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis(\"off\")  # Hide axes for better visualization\n",
    "            axes[i].set_title(f\"Score: {linkedin_score:.2f}\")\n",
    "    else:\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(\"No Score\")\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(len(image_files), len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b1ec717-1d94-40bd-8221-32b4b0161f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import mediapipe\n",
    "import math\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Define the path to your local directory\n",
    "output_directory = \"/dbfs/FileStore/all_images_combined\"\n",
    "\n",
    "# -------------\n",
    "# HELPER LOAD FUNCTION\n",
    "# -------------\n",
    "\n",
    "def load_local_image(image_filename):\n",
    "    \"\"\"\n",
    "    Load an image from the local directory `output_directory`.\n",
    "    Returns a NumPy array (BGR) suitable for OpenCV.\n",
    "    \"\"\"\n",
    "    # Construct the full path\n",
    "    full_path = os.path.join(output_directory, image_filename)\n",
    "    # Use PIL to open, then convert to NumPy (as OpenCV usually uses BGR)\n",
    "    pil_image = Image.open(full_path).convert(\"RGB\")\n",
    "    img_array = np.array(pil_image)\n",
    "    # Convert RGB (PIL) to BGR (OpenCV)\n",
    "    img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
    "    return img_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_single_feature(image_filename, prompt_yes, prompt_no):\n",
    "    \"\"\"\n",
    "    Check a single feature using CLIP text-image similarity.\n",
    "\n",
    "    Args:\n",
    "        image_filename (str): The file name of the image (not URL).\n",
    "        prompt_yes (str): Positive prompt text.\n",
    "        prompt_no (str): Negative prompt text.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing prompt_yes score, prompt_no score, final_decision.\n",
    "    \"\"\"\n",
    "    # Load image from local path\n",
    "    full_path = os.path.join(output_directory, image_filename)\n",
    "    pil_img = Image.open(full_path).convert(\"RGB\")\n",
    "    \n",
    "    # Preprocess for CLIP and move to device\n",
    "    image = Clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Prepare prompts\n",
    "    text_prompts = [prompt_yes, prompt_no]\n",
    "    text = clip.tokenize(text_prompts).to(device)\n",
    "\n",
    "    # Compute similarity\n",
    "    with torch.no_grad():\n",
    "        image_features = Clip_model.encode_image(image)\n",
    "        text_features = Clip_model.encode_text(text)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    scores = similarities.cpu().numpy().flatten()\n",
    "    result = {\n",
    "        \"prompt_yes\": f\"{prompt_yes}: {scores[0]:.2f}%\",\n",
    "        \"prompt_no\": f\"{prompt_no}: {scores[1]:.2f}%\",\n",
    "        \"final_decision\": \"Yes\" if scores[0] > scores[1] else \"No\"\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "# -------------\n",
    "# BODY PROPORTION\n",
    "# -------------\n",
    "\n",
    "def calculate_body_proportion(image_filename):\n",
    "    \"\"\"\n",
    "    Calculate the proportion of the body (as detected by MediaPipe Pose) relative to the image size.\n",
    "    \"\"\"\n",
    "    import mediapipe as mp\n",
    "\n",
    "    # Load local image\n",
    "    img_array = load_local_image(image_filename)\n",
    "    h, w, _ = img_array.shape\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5) as pose:\n",
    "        results = pose.process(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            print(\"No body detected.\")\n",
    "            return 0\n",
    "\n",
    "        body_landmarks = results.pose_landmarks.landmark\n",
    "        x_coords = [min(max(lm.x, 0), 1) * w for lm in body_landmarks]\n",
    "        y_coords = [min(max(lm.y, 0), 1) * h for lm in body_landmarks]\n",
    "\n",
    "        x_min, x_max = min(x_coords), max(x_coords)\n",
    "        y_min, y_max = min(y_coords), max(y_coords)\n",
    "\n",
    "        body_width = x_max - x_min\n",
    "        body_height = y_max - y_min\n",
    "        body_area = body_width * body_height\n",
    "        image_area = w * h\n",
    "\n",
    "        proportion = body_area / image_area\n",
    "        return proportion\n",
    "\n",
    "\n",
    "# -------------\n",
    "# NOSE CENTER SCORE\n",
    "# -------------\n",
    "\n",
    "def calculate_nose_center_score(image_filename):\n",
    "    \"\"\"\n",
    "    Calculate how close the nose (landmark) is to the image center in x-dimension.\n",
    "    \"\"\"\n",
    "    import dlib\n",
    "    import mediapipe as mp\n",
    "\n",
    "    # Load local image as PIL\n",
    "    full_path = os.path.join(output_directory, image_filename)\n",
    "    try:\n",
    "        pil_img = Image.open(full_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load image from local path: {e}\")\n",
    "\n",
    "    img_array = np.array(pil_img)\n",
    "    img_height, img_width = img_array.shape[:2]\n",
    "    image_center_x = img_width / 2\n",
    "\n",
    "    # Try Dlib first\n",
    "    try:\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor(dlib.data_file(\"shape_predictor_68_face_landmarks.dat\"))\n",
    "        faces = detector(img_array, 1)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            face = faces[0]\n",
    "            landmarks = predictor(img_array, face)\n",
    "            # Landmark 30 is the nose tip\n",
    "            nose_x = landmarks.part(30).x\n",
    "\n",
    "            normalized_distance_x = abs(nose_x - image_center_x) / img_width\n",
    "            return normalized_distance_x\n",
    "    except Exception:\n",
    "        print(\"Dlib failed to detect nose. Trying with MediaPipe...\")\n",
    "\n",
    "    # Fallback to MediaPipe\n",
    "    try:\n",
    "        mp_face_mesh = mp.solutions.face_mesh\n",
    "        with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh:\n",
    "            results = face_mesh.process(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            if not results.multi_face_landmarks:\n",
    "                return 0\n",
    "\n",
    "            # Nose tip index in MediaPipe is 1\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            nose_x = face_landmarks.landmark[1].x * img_width\n",
    "\n",
    "            normalized_distance_x = abs(nose_x - image_center_x)\n",
    "            return normalized_distance_x\n",
    "    except Exception as e:\n",
    "        print(f\"MediaPipe failed: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "# -------------\n",
    "# FACE PROPORTIONS\n",
    "# -------------\n",
    "\n",
    "def calculate_face_proportions(image_filename):\n",
    "    \"\"\"\n",
    "    Calculate what fraction of the image area is occupied by the detected face(s).\n",
    "    \"\"\"\n",
    "    import mediapipe as mp\n",
    "\n",
    "    # Load local image\n",
    "    img_array = load_local_image(image_filename)\n",
    "    h, w, _ = img_array.shape\n",
    "\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "        results = face_detection.process(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if not results.detections:\n",
    "            return 0\n",
    "\n",
    "        total_face_area = 0\n",
    "        for detection in results.detections:\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            x, y, width, height = bboxC.xmin, bboxC.ymin, bboxC.width, bboxC.height\n",
    "            total_face_area += (width * w) * (height * h)\n",
    "\n",
    "        image_area = h * w\n",
    "        proportion = total_face_area / image_area\n",
    "        return proportion\n",
    "\n",
    "\n",
    "# -------------\n",
    "# CENTER SCORE Y-AXIS\n",
    "# -------------\n",
    "\n",
    "def calculate_center_score_y_axis(image_filename):\n",
    "    \"\"\"\n",
    "    Returns how close the nose tip is to the vertical center of the image (score 0-1).\n",
    "    \"\"\"\n",
    "    import mediapipe as mp\n",
    "\n",
    "    img_array = load_local_image(image_filename)\n",
    "    h, w, _ = img_array.shape\n",
    "\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh:\n",
    "        results = face_mesh.process(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "        if not results.multi_face_landmarks:\n",
    "            return 0\n",
    "\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        nose_tip_index = 1  # nose tip\n",
    "        nose_tip_y = face_landmarks.landmark[nose_tip_index].y * h\n",
    "        image_center_y = h / 2\n",
    "\n",
    "        normalized_distance_y = abs(nose_tip_y - image_center_y) / (h / 2)\n",
    "        score = max(0, 1 - normalized_distance_y)\n",
    "        return score\n",
    "\n",
    "\n",
    "# -------------\n",
    "# CENTER SCORE X-AXIS\n",
    "# -------------\n",
    "\n",
    "def calculate_center_score_x_axis(image_filename):\n",
    "    \"\"\"\n",
    "    Returns how centered the eye region is horizontally (score 0-1).\n",
    "    \"\"\"\n",
    "    import mediapipe as mp\n",
    "\n",
    "    img_array = load_local_image(image_filename)\n",
    "    h, w, _ = img_array.shape\n",
    "\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh:\n",
    "        results = face_mesh.process(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "        if not results.multi_face_landmarks:\n",
    "            return 0\n",
    "\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        left_eye_indices = [33, 160, 158]\n",
    "        right_eye_indices = [362, 385, 387]\n",
    "\n",
    "        left_eye = np.mean([(face_landmarks.landmark[i].x * w,\n",
    "                             face_landmarks.landmark[i].y * h) for i in left_eye_indices], axis=0)\n",
    "        right_eye = np.mean([(face_landmarks.landmark[i].x * w,\n",
    "                              face_landmarks.landmark[i].y * h) for i in right_eye_indices], axis=0)\n",
    "\n",
    "        image_center_x = w / 2\n",
    "        eye_center_x = (left_eye[0] + right_eye[0]) / 2\n",
    "\n",
    "        normalized_distance = abs(eye_center_x - image_center_x) / (w / 2)\n",
    "        score = max(0, 1 - normalized_distance)\n",
    "        return score\n",
    "\n",
    "\n",
    "# -------------\n",
    "# TILT HEAD\n",
    "# -------------\n",
    "\n",
    "def calculate_tilt_head(image_filename):\n",
    "    \"\"\"\n",
    "    Calculate how tilted the head is (in degrees) based on the eye line.\n",
    "    Returns a normalized tilt score (0-??).\n",
    "    \"\"\"\n",
    "    import mediapipe as mp\n",
    "\n",
    "    img_array = load_local_image(image_filename)\n",
    "    h, w, _ = img_array.shape\n",
    "\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh:\n",
    "        results = face_mesh.process(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "        if not results.multi_face_landmarks:\n",
    "            return 0\n",
    "\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        left_eye_indices = [33, 160, 158]\n",
    "        right_eye_indices = [362, 385, 387]\n",
    "\n",
    "        left_eye = np.mean([(face_landmarks.landmark[i].x * w,\n",
    "                             face_landmarks.landmark[i].y * h) for i in left_eye_indices], axis=0)\n",
    "        right_eye = np.mean([(face_landmarks.landmark[i].x * w,\n",
    "                              face_landmarks.landmark[i].y * h) for i in right_eye_indices], axis=0)\n",
    "\n",
    "        delta_x = right_eye[0] - left_eye[0]\n",
    "        delta_y = right_eye[1] - left_eye[1]\n",
    "        tilt_angle_radians = np.arctan2(delta_y, delta_x)\n",
    "        tilt_angle_degrees = np.degrees(tilt_angle_radians)\n",
    "\n",
    "        #  normalization: compare to a max tilt of 30 degrees\n",
    "        max_tilt = 30.0\n",
    "        normalized_tilt_score = max(0, abs(tilt_angle_degrees) / max_tilt)\n",
    "        return normalized_tilt_score\n",
    "\n",
    "\n",
    "# -------------\n",
    "# LOOKING DIRECTION\n",
    "# -------------\n",
    "\n",
    "def calculate_looking_direction_score(image_filename):\n",
    "    \"\"\"\n",
    "    Estimate if the person is looking at the camera (eye alignment with nose).\n",
    "    Returns a score 0-1 (1 = direct look).\n",
    "    \"\"\"\n",
    "    import mediapipe as mp\n",
    "\n",
    "    img_array = load_local_image(image_filename)\n",
    "    h, w, _ = img_array.shape\n",
    "\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh:\n",
    "        results = face_mesh.process(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))\n",
    "        if not results.multi_face_landmarks:\n",
    "            return 0\n",
    "\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        left_eye_indices = [33, 160, 158]\n",
    "        right_eye_indices = [362, 385, 387]\n",
    "        nose_tip_index = 1\n",
    "\n",
    "        left_eye = np.mean([(face_landmarks.landmark[i].x * w,\n",
    "                             face_landmarks.landmark[i].y * h) for i in left_eye_indices], axis=0)\n",
    "        right_eye = np.mean([(face_landmarks.landmark[i].x * w,\n",
    "                              face_landmarks.landmark[i].y * h) for i in right_eye_indices], axis=0)\n",
    "        nose_tip = face_landmarks.landmark[nose_tip_index]\n",
    "        nose_tip = (nose_tip.x * w, nose_tip.y * h)\n",
    "\n",
    "        eye_center_x = (left_eye[0] + right_eye[0]) / 2\n",
    "        delta_x = abs(nose_tip[0] - eye_center_x)\n",
    "\n",
    "        # Example threshold for 0 score:\n",
    "        max_delta = w * 0.05\n",
    "        normalized_score = max(0, 1 - (delta_x / max_delta))\n",
    "        return normalized_score\n",
    "\n",
    "\n",
    "# -------------\n",
    "# FACE COORD DETECTION\n",
    "# -------------\n",
    "\n",
    "def detect_face_coordinates(image_filename):\n",
    "    \"\"\"\n",
    "    Detect face coordinates using Mediapipe. Return (x_min, y_min, x_max, y_max).\n",
    "    \"\"\"\n",
    "    import mediapipe as mp\n",
    "\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    face_detector = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "    # Load local image\n",
    "    pil_img = Image.open(os.path.join(output_directory, image_filename)).convert(\"RGB\")\n",
    "    img_array = np.array(pil_img)\n",
    "\n",
    "    results = face_detector.process(img_array)\n",
    "    if results.detections:\n",
    "        detection = results.detections[0]\n",
    "        bboxC = detection.location_data.relative_bounding_box\n",
    "        h, w, _ = img_array.shape\n",
    "        x_min = int(bboxC.xmin * w)\n",
    "        y_min = int(bboxC.ymin * h)\n",
    "        x_max = x_min + int(bboxC.width * w)\n",
    "        y_max = y_min + int(bboxC.height * h)\n",
    "        return (x_min, y_min, x_max, y_max)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------\n",
    "# DPI SCORE\n",
    "# -------------\n",
    "\n",
    "def calculate_dpi_score(image_filename):\n",
    "    \"\"\"\n",
    "    Approximate resolution-based score.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "\n",
    "    img = Image.open(os.path.join(output_directory, image_filename))\n",
    "    width, height = img.size\n",
    "    total_pixels = width * height\n",
    "\n",
    "    # Example base: 480 x 360 (172800 pixels)\n",
    "    max_pixels = 480 * 360\n",
    "    normalized_score = min(total_pixels / max_pixels, 1)\n",
    "    return normalized_score\n",
    "\n",
    "\n",
    "# -------------\n",
    "# BACKGROUND BRIGHTNESS\n",
    "# -------------\n",
    "\n",
    "def calculate_background_brightness(image_filename):\n",
    "    \"\"\"\n",
    "    Estimate overall brightness of the image. Returns a normalized brightness (0-1).\n",
    "    \"\"\"\n",
    "    img_array = load_local_image(image_filename)\n",
    "\n",
    "    # Convert BGR to RGB for standard brightness formula\n",
    "    rgb_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
    "    # brightness ~ 0.299*R + 0.587*G + 0.114*B\n",
    "    brightness = 0.299 * rgb_array[:, :, 0] + \\\n",
    "                 0.587 * rgb_array[:, :, 1] + \\\n",
    "                 0.114 * rgb_array[:, :, 2]\n",
    "\n",
    "    overall_brightness = np.mean(brightness)\n",
    "    # Example normalization factor\n",
    "    X = 180.0\n",
    "    normalized_brightness = overall_brightness / X\n",
    "    clamped_brightness = min(normalized_brightness, 1)\n",
    "    return clamped_brightness\n",
    "\n",
    "\n",
    "# -------------\n",
    "# MAIN FEATURE SCORES AGGREGATOR\n",
    "# -------------\n",
    "\n",
    "def feature_scores(image_filename):\n",
    "    \"\"\"\n",
    "    Runs a set of checks on the image file and returns a dictionary of scores.\n",
    "    \"\"\"\n",
    "    feature_checks = [\n",
    "        {\"name\": \"smile\", \"prompt_yes\": \"smile\", \"prompt_no\": \"Else\"},\n",
    "        {\"name\": \"wear professional Attire\", \"prompt_yes\": \"person with professional Attire\", \"prompt_no\": \"Else\"},\n",
    "        {\"name\": \"use clean background\", \"prompt_yes\": \"clean background\", \"prompt_no\": \"aggressive background that steals all the attention\"},\n",
    "        {\"name\": \"look serious\", \"prompt_yes\": \"serious\", \"prompt_no\": \"Silly\"},\n",
    "        {\"name\": \"show your face clearly\", \"prompt_yes\": \"clearly contains a face\", \"prompt_no\": \"Else\"},\n",
    "        {\"name\": \"use formal expression\", \"prompt_yes\": \"job expressions\", \"prompt_no\": \"Else\"},\n",
    "        {\"name\": \"wear suit\", \"prompt_yes\": \"person with suit\", \"prompt_no\": \"casual outfit\"},\n",
    "        {\"name\": \"look confidence\", \"prompt_yes\": \"confident person\", \"prompt_no\": \"Else\"},\n",
    "        {\"name\": \"look approachability\", \"prompt_yes\": \"friendly\", \"prompt_no\": \"Else\"},\n",
    "        {\"name\": \"don't look emotional\", \"prompt_yes\": \"neutral expression\", \"prompt_no\": \"emotional expression\"},\n",
    "        {\"name\": \"crop the picture\", \"prompt_yes\": \"good image frame/lines\", \"prompt_no\": \"aggressively cut image\"},\n",
    "        {\"name\": \"avoid noise or artifacts\", \"prompt_yes\": \"no noise and artifacts\", \"prompt_no\": \"Else\"},\n",
    "        {\"name\": \"use positive atmosphere\", \"prompt_yes\": \"positive atmosphere\", \"prompt_no\": \"Negative atmosphere\"},\n",
    "        {\"name\": \"use corporate environment\", \"prompt_yes\": \"office\", \"prompt_no\": \"Else\"},\n",
    "        {\"name\": \"look trustworthiness\", \"prompt_yes\": \"trustworthy\", \"prompt_no\": \"untrust\"},\n",
    "        {\"name\": \"be energetic\", \"prompt_yes\": \"positive energey\", \"prompt_no\": \"unwelcoming\"},\n",
    "        {\"name\": \"look distinctive\", \"prompt_yes\": \"distinct and memorable\", \"prompt_no\": \"Regular\"},\n",
    "        {\"name\": \"look calm\", \"prompt_yes\": \"calm\", \"prompt_no\": \"Upset\"},\n",
    "        {\"name\": \"fix your hair\", \"prompt_yes\": \"neat hair\", \"prompt_no\": \"messy hair\"},\n",
    "        {\"name\": \"show your face more\", \"prompt_yes\": \"whole face\", \"prompt_no\": \"cover face\"},\n",
    "        {\"name\": \"show your body more\", \"prompt_yes\": \"head and shoulder\", \"prompt_no\": \"cant see shoulder\"},\n",
    "        {\"name\": \"put your face horizontally in the center\", \"prompt_yes\": \"person in middle\", \"prompt_no\": \"too far to the side\"},\n",
    "        {\"name\": \"put your face vertically in the center\", \"prompt_yes\": \"person in middle\", \"prompt_no\": \"too far vertucally\"},\n",
    "        {\"name\": \"tilt your head\", \"prompt_yes\": \"straight head\", \"prompt_no\": \"Else\"},\n",
    "        {\"name\": \"use eye contact\", \"prompt_yes\": \"looking to the camera\", \"prompt_no\": \"eyes off camera\"},\n",
    "        {\"name\": \"use high resolution\", \"prompt_yes\": \"clear\", \"prompt_no\": \"blurry\"},\n",
    "        {\"name\": \"use good lighting\", \"prompt_yes\": \"good lighting\", \"prompt_no\": \"bad lighting\"},\n",
    "        {\"name\": \"use color harmony in the pic\", \"prompt_yes\": \"Colorful image\", \"prompt_no\": \"Monochromatic image\"},\n",
    "      \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    scores = {}\n",
    "    all_results = {}\n",
    "\n",
    "    # 1) Use CLIP-based checks\n",
    "    for check in feature_checks:\n",
    "      \n",
    "        result = check_single_feature(image_filename, check[\"prompt_yes\"], check[\"prompt_no\"])\n",
    "        all_results[check[\"name\"]] = result\n",
    "\n",
    "    # Extract the \"Yes\" percentage for each feature\n",
    "    for feature_name, result in all_results.items():\n",
    "        yes_score_str = result[\"prompt_yes\"].split(\":\")[1].strip().replace('%', '')\n",
    "        yes_score = float(yes_score_str)\n",
    "        scores[feature_name] = yes_score\n",
    "\n",
    "    # 2) If face_proportion is not 0, override or supplement that feature\n",
    "    fp = calculate_face_proportions(image_filename)\n",
    "    if fp != 0:\n",
    "        scores[\"show your face more\"] = fp\n",
    "\n",
    "    bp = calculate_body_proportion(image_filename)\n",
    "    if bp != 0:\n",
    "        scores[\"show your body more\"] = bp\n",
    "\n",
    "    cscore_x = calculate_center_score_x_axis(image_filename)\n",
    "    if cscore_x != 0:\n",
    "        scores[\"put your face horizontally in the center\"] = cscore_x\n",
    "\n",
    "    cscore_y = calculate_center_score_y_axis(image_filename)\n",
    "    if cscore_y != 0:\n",
    "        scores[\"put your face vertically in the center\"] = cscore_y\n",
    "\n",
    "    tilt = calculate_tilt_head(image_filename)\n",
    "    if tilt != 0:\n",
    "        scores[\"tilt your head\"] = tilt\n",
    "\n",
    "    looking_dir = calculate_looking_direction_score(image_filename)\n",
    "    if looking_dir != 0:\n",
    "        scores[\"use eye contact\"] = looking_dir\n",
    "\n",
    "    # 3) Additional numeric scores\n",
    "    scores[\"use high resolution\"] = calculate_dpi_score(image_filename)\n",
    "    scores[\"use lighting quality\"] = calculate_background_brightness(image_filename)\n",
    "    # If you had a colorfulness or sharpness measure, you can add them similarly.\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff8fb5c9-d971-4b6e-b9bc-8eef5342c906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow logging messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Optional: Set Python logging level to WARNING or ERROR\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Ignore specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Feedback manager requires a model with a single signature inference.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "217fab80-2069-4b98-80da-8305f9db85b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "# Suppress messages from the feedback manager\n",
    "logging.getLogger(\"inference_feedback_manager\").setLevel(logging.ERROR)\n",
    "\n",
    "output_directory = \"/dbfs/FileStore/profile_pictures_20250118T091222Z_001/profile_pictures\" #profile pictures scrap link\n",
    "\n",
    "# Gather all files in the directory\n",
    "all_files = os.listdir(output_directory)\n",
    "\n",
    "# (Optional) filter to only include typical image file extensions\n",
    "valid_extensions = {\".jpg\", \".png\"}\n",
    "image_files = [f for f in all_files if os.path.splitext(f)[1].lower() in valid_extensions]\n",
    "\n",
    "# Instead of taking the first 10 images, start from index 1000 and grab the next 8000\n",
    "image_files = image_files[:8000]\n",
    "\n",
    "data = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Iterate over those images\n",
    "for filename in image_files:\n",
    "    full_path = os.path.join(output_directory, filename)\n",
    "\n",
    "    print('iteration', i)\n",
    "    i = i+1\n",
    "    try:\n",
    "        # Verify if the file is a valid image\n",
    "        with Image.open(full_path) as img:\n",
    "            img.verify()\n",
    "        \n",
    "        # 1) Compute LinkedIn score for the image\n",
    "        linkedin_score = calculate_linkedin_score(full_path, Clip_model, Clip_preprocess, device)\n",
    "        \n",
    "        # 2) Compute feature scores\n",
    "        scores = feature_scores(full_path)\n",
    "        \n",
    "        # 3) Attach the LinkedIn score as 'LABEL' and save\n",
    "        scores[\"LABEL\"] = linkedin_score\n",
    "        data.append(scores)\n",
    "        \n",
    "    except UnidentifiedImageError:\n",
    "        logging.error(f\"Cannot identify image file {full_path}\")\n",
    "\n",
    "print(\"Total images processed:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ecca38-cf15-47ea-b803-7fa04b955a5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the data list to a DataFrame\n",
    "Image_processed_df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the output file path\n",
    "output_csv_path = \"/dbfs/FileStore/union_df_1.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "Image_processed_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Data saved successfully to {output_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PreProcess images and features 2025-01-26 23:36:07",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}